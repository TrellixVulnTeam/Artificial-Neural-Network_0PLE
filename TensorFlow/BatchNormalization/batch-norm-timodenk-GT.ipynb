{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://timodenk.com/blog/tensorflow-batch-normalization/\n",
    "https://github.com/Simsso/Machine-Learning-Tinkering/blob/master/snippets/batch-norm.ipynb   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.995004]\n",
      " [ 0.      ]\n",
      " [ 9.995004]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The batch normalization layer does not normalize based on the current batch if its training parameter is not set to true.\n",
    "\"\"\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 1], 'x')\n",
    "y = tf.layers.batch_normalization(x)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "y_out = sess.run(y, feed_dict={x: [[-10], [0], [10]]})\n",
    "sess.close()\n",
    "\n",
    "print(y_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2247357]\n",
      " [ 0.       ]\n",
      " [ 1.2247357]]\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "With this setup, the batch normalization layer looks at the current batch and normalized it depending on its value.\n",
    "\"\"\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 1], 'x')\n",
    "y = tf.layers.batch_normalization(x, training=True)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "y_out = sess.run(y, feed_dict={x: [[-10], [0], [10]]})\n",
    "sess.close()\n",
    "\n",
    "print(y_out)\n",
    "###################################################3\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 1], 'x')\n",
    "y = tf.layers.batch_normalization(x, training=True)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "y_out = sess.run(y, feed_dict={x: [[-10]]})\n",
    "sess.close()\n",
    "\n",
    "print(y_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'batch_normalization/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'batch_normalization/AssignMovingAvg_1' type=AssignSub>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2247357]\n",
      " [ 0.       ]\n",
      " [ 1.2247357]]\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In order to update the two moving average variables (mean and variance),\n",
    "which the tf.layers.batch_normalization function call creates automatically,\n",
    "two operations must be evaluated while feeding a batch through the layer.\n",
    "\"\"\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 1], 'x')\n",
    "y = tf.layers.batch_normalization(x, training=True)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "y_1 = sess.run([y, update_ops], feed_dict={x: [[-10], [0], [10]]})[0]\n",
    "y_2 = sess.run(y, feed_dict={x: [[-10]]})\n",
    "sess.close()\n",
    "\n",
    "print(y_1)\n",
    "print(y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2247357]\n",
      " [ 0.       ]\n",
      " [ 1.2247357]]\n",
      "[[-7.766966]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The values for y1 and y2 remain the same.\n",
    "That is because the moving averages are only being used, if the training parameter is set to False.\n",
    "We can control it with a placeholder (here a placeholder with a default value) and set it to True when feeding the larger batch\n",
    "(and False for the smaller batch; strictly not necessary because it is the placeholder’s default value anyways)\n",
    "\n",
    "GT:\n",
    "When `training` is set to True tf.layers.batch_normalization normalize.\n",
    "When `training` is set to False tf.layers.batch_normalization does not normalize and use moving averages.\n",
    "So, for larger batch set `training=True` and for small batch set `training=False`.\n",
    "\"\"\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training')\n",
    "x = tf.placeholder(tf.float32, [None, 1], 'x')\n",
    "y = tf.layers.batch_normalization(x, training=is_training)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "y_1 = sess.run([y, update_ops], feed_dict={x: [[-10], [0], [10]], is_training: True})[0]\n",
    "#y_1 = sess.run(y, feed_dict={x: [[-10], [0], [10]], is_training: True}) # without update_ops\n",
    "y_2 = sess.run(y, feed_dict={x: [[-10]], is_training: False})\n",
    "sess.close()\n",
    "\n",
    "print(y_1)\n",
    "print(y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2247357]\n",
      " [ 0.       ]\n",
      " [ 1.2247357]]\n",
      "[[-1.224762]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Kind of weird. It’s neither 0, which it was without moving averages, nor -1.22,\n",
    "which it should be if it was normalized with the same factors as the X1 batch.\n",
    "\n",
    "The reason for the wrong normalization of the small batch is that the moving averages update slowly.\n",
    "If we were to feed the larger batch multiple times, the second batch would be properly normalized:\n",
    "\"\"\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training')\n",
    "x = tf.placeholder(tf.float32, [None, 1], 'x')\n",
    "y = tf.layers.batch_normalization(x, training=is_training)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "for _ in range(1000):\n",
    "    y_1 = sess.run([y, update_ops], feed_dict={x: [[-10], [0], [10]], is_training: True})[0]\n",
    "y_2 = sess.run(y, feed_dict={x: [[-10]], is_training: False})\n",
    "sess.close()\n",
    "\n",
    "print(y_1)\n",
    "print(y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2247357]\n",
      " [ 0.       ]\n",
      " [ 1.2247357]]\n",
      "[[-1.224762]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Right now we have to call sess.run and pass the update_ops manually.\n",
    "It is more convenient to add them as a control dependency,\n",
    "such that TensorFlow always executes them if the Tensor y is being evaluated.\n",
    "\"\"\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training')\n",
    "x = tf.placeholder(tf.float32, [None, 1], 'x')\n",
    "y = tf.layers.batch_normalization(x, training=is_training)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    y = tf.identity(y)\n",
    "    \n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "x_1 = [[-10], [0], [10]]\n",
    "x_2 = [[-10]]\n",
    "for _ in range(1000):\n",
    "    y_1 = sess.run(y, feed_dict={x: x_1, is_training: True})\n",
    "y_2 = sess.run(y, feed_dict={x: x_2})\n",
    "\n",
    "print(y_1)\n",
    "print(y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'batch_normalization/gamma:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'batch_normalization/beta:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'batch_normalization/moving_mean:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'batch_normalization/moving_variance:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Typically, is_training should be set to True during training and False when performing inference.\n",
    "The values stored by the batch normalization layer can be examined\n",
    "\"\"\"\n",
    "\n",
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"\", reuse=tf.AUTO_REUSE):\n",
    "    out = sess.run([tf.get_variable('batch_normalization/moving_mean'),\n",
    "                    tf.get_variable('batch_normalization/moving_variance')])\n",
    "    moving_average, moving_variance = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moving_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66.66382], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moving_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
