{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../Datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../Datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../Datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../Datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../Datasets/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000 10000\n"
     ]
    }
   ],
   "source": [
    "trN, teN = mnist.train.num_examples, mnist.test.num_examples\n",
    "print(trN, teN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train\")\n",
    "print(trX.shape)\n",
    "#print(trX[0])\n",
    "print(trY.shape)\n",
    "print(trY[0])\n",
    "\n",
    "print(\"Test\")\n",
    "print(teX.shape)\n",
    "print(teY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "trX = trX.reshape(-1, 28, 28) # (55000, 28, 28)\n",
    "print(trX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21fc0e688d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prints the image\n",
    "Input: image pixels in list\n",
    "\"\"\"\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def print_image(image, title):\n",
    "    plt.imshow(image, cmap=plt.cm.gray)\n",
    "    plt.title(title)\n",
    "    #plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "print_image(trX[5700], \"Original Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # GT: why Normalize??\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../Datasets/CIFAR-10', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../Datasets/CIFAR-10', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "12500\n",
      "4\n",
      "3\n",
      "32\n",
      "32\n",
      "torch.FloatTensor\n",
      "torch.Size([32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Data is loaded in batches and each batch contains 4 images.\n",
    "\n",
    "print(\"Train\")\n",
    "print(len(trainloader)) # 12,500 batches (50,000 images)\n",
    "dataiter = iter(trainloader)\n",
    "row1 = dataiter.next()\n",
    "print(len(row1[0])) # 4 # batch_size # It has the 4 labels between 0 to 9 -- ??? Should be images\n",
    "print(len(row1[0][0])) # 3 # 3-channel color \n",
    "print(len(row1[0][0][0])) # 32 pixels\n",
    "print(len(row1[0][0][0][0])) # 32 pixels\n",
    "print(row1[0][0][0].type())\n",
    "print(row1[0][0][0].size())\n",
    "\n",
    "#print(trX[0])\n",
    "# print(trY.shape)\n",
    "# print(trY[0])\n",
    "\n",
    "# print(\"Test\")\n",
    "# print(teX.shape)\n",
    "# print(teY.shape)\n",
    "\n",
    "# print(len(trainloader))\n",
    "# dataiter = iter(trainloader)\n",
    "# row1 = dataiter.next()\n",
    "\n",
    "# print(len(row1[0])) # 4 # batch_size # It has the 4 labels between 0 to 9\n",
    "# print(len(row1[0][0])) # 3 # 3-channel color \n",
    "# print(len(row1[0][0][0])) # 32 pixels\n",
    "# print(len(row1[0][0][0][0])) # 32 pixels\n",
    "# print(row1[0][0][0].type())\n",
    "# print(row1[0][0][0].size())\n",
    "\n",
    "# print(\"\")\n",
    "# print(len(testloader))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process images of this size. Note that this differs from the original CIFAR\n",
    "# image size of 32 x 32. If one alters this number, then the entire model\n",
    "# architecture will change and any model would need to be retrained.\n",
    "IMAGE_SIZE = 24\n",
    "\n",
    "# Global constants describing the CIFAR-10 data set.\n",
    "NUM_CLASSES = 10\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, tarfile\n",
    "import tensorflow as tf\n",
    "from six.moves import urllib\n",
    "\n",
    "def maybe_download_and_extract():\n",
    "    \"\"\"Download and extract the tarball from Alex's website.\"\"\"\n",
    "    #dest_directory = 'data/cifar10_data'\n",
    "    dest_directory = '../Datasets/CIFAR-10'\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "        filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, reporthook=_progress)\n",
    "        print()\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "        tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n",
    "\n",
    "maybe_download_and_extract()\n",
    "\n",
    "def read_cifar10(filename_queue):\n",
    "  \"\"\"Reads and parses examples from CIFAR10 data files.\n",
    "  Recommendation: if you want N-way read parallelism, call this function\n",
    "  N times.  This will give you N independent Readers reading different\n",
    "  files & positions within those files, which will give better mixing of\n",
    "  examples.\n",
    "  Args:\n",
    "    filename_queue: A queue of strings with the filenames to read from.\n",
    "  Returns:\n",
    "    An object representing a single example, with the following fields:\n",
    "      height: number of rows in the result (32)\n",
    "      width: number of columns in the result (32)\n",
    "      depth: number of color channels in the result (3)\n",
    "      key: a scalar string Tensor describing the filename & record number\n",
    "        for this example.\n",
    "      label: an int32 Tensor with the label in the range 0..9.\n",
    "      uint8image: a [height, width, depth] uint8 Tensor with the image data\n",
    "  \"\"\"\n",
    "\n",
    "  class CIFAR10Record(object):\n",
    "    pass\n",
    "  result = CIFAR10Record()\n",
    "\n",
    "  # Dimensions of the images in the CIFAR-10 dataset.\n",
    "  # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the\n",
    "  # input format.\n",
    "  label_bytes = 1  # 2 for CIFAR-100\n",
    "  result.height = 32\n",
    "  result.width = 32\n",
    "  result.depth = 3\n",
    "  image_bytes = result.height * result.width * result.depth\n",
    "  # Every record consists of a label followed by the image, with a\n",
    "  # fixed number of bytes for each.\n",
    "  record_bytes = label_bytes + image_bytes\n",
    "\n",
    "  # Read a record, getting filenames from the filename_queue.  No\n",
    "  # header or footer in the CIFAR-10 format, so we leave header_bytes\n",
    "  # and footer_bytes at their default of 0.\n",
    "  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n",
    "  result.key, value = reader.read(filename_queue)\n",
    "\n",
    "  # Convert from a string to a vector of uint8 that is record_bytes long.\n",
    "  record_bytes = tf.decode_raw(value, tf.uint8)\n",
    "\n",
    "  # The first bytes represent the label, which we convert from uint8->int32.\n",
    "  result.label = tf.cast(\n",
    "      tf.slice(record_bytes, [0], [label_bytes]), tf.int32)\n",
    "\n",
    "  # The remaining bytes after the label represent the image, which we reshape\n",
    "  # from [depth * height * width] to [depth, height, width].\n",
    "  depth_major = tf.reshape(tf.slice(record_bytes, [label_bytes], [image_bytes]),\n",
    "                           [result.depth, result.height, result.width])\n",
    "  # Convert from [depth, height, width] to [height, width, depth].\n",
    "  result.uint8image = tf.transpose(depth_major, [1, 2, 0])\n",
    "\n",
    "  return result\n",
    "\n",
    "def _generate_image_and_label_batch(image, label, min_queue_examples,\n",
    "                                    batch_size):\n",
    "  \"\"\"Construct a queued batch of images and labels.\n",
    "  Args:\n",
    "    image: 3-D Tensor of [height, width, 3] of type.float32.\n",
    "    label: 1-D Tensor of type.int32\n",
    "    min_queue_examples: int32, minimum number of samples to retain\n",
    "      in the queue that provides of batches of examples.\n",
    "    batch_size: Number of images per batch.\n",
    "  Returns:\n",
    "    images: Images. 4D tensor of [batch_size, height, width, 3] size.\n",
    "    labels: Labels. 1D tensor of [batch_size] size.\n",
    "  \"\"\"\n",
    "  # Create a queue that shuffles the examples, and then\n",
    "  # read 'batch_size' images + labels from the example queue.\n",
    "  num_preprocess_threads = 16\n",
    "  images, label_batch = tf.train.shuffle_batch(\n",
    "      [image, label],\n",
    "      batch_size=batch_size,\n",
    "      num_threads=num_preprocess_threads,\n",
    "      capacity=min_queue_examples + 3 * batch_size,\n",
    "      min_after_dequeue=min_queue_examples)\n",
    "\n",
    "  # Display the training images in the visualizer.\n",
    "\n",
    "  return images, tf.reshape(label_batch, [batch_size])\n",
    "\n",
    "def inputs(eval_data, data_dir, batch_size):\n",
    "  \"\"\"Construct input for CIFAR evaluation using the Reader ops.\n",
    "  Args:\n",
    "    eval_data: bool, indicating if one should use the train or eval data set.\n",
    "    data_dir: Path to the CIFAR-10 data directory.\n",
    "    batch_size: Number of images per batch.\n",
    "  Returns:\n",
    "    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n",
    "    labels: Labels. 1D tensor of [batch_size] size.\n",
    "  \"\"\"\n",
    "  if not eval_data:\n",
    "    filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)\n",
    "                 for i in xrange(1, 6)]\n",
    "    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "  else:\n",
    "    filenames = [os.path.join(data_dir, 'test_batch.bin')]\n",
    "    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n",
    "\n",
    "  for f in filenames:\n",
    "    if not tf.gfile.Exists(f):\n",
    "      raise ValueError('Failed to find file: ' + f)\n",
    "\n",
    "  # Create a queue that produces the filenames to read.\n",
    "  filename_queue = tf.train.string_input_producer(filenames)\n",
    "\n",
    "  # Read examples from files in the filename queue.\n",
    "  read_input = read_cifar10(filename_queue)\n",
    "  reshaped_image = tf.cast(read_input.uint8image, tf.float32)\n",
    "\n",
    "  height = IMAGE_SIZE\n",
    "  width = IMAGE_SIZE\n",
    "\n",
    "  # Image processing for evaluation.\n",
    "  # Crop the central [height, width] of the image.\n",
    "  resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,\n",
    "                                                         width, height)\n",
    "\n",
    "  # Subtract off the mean and divide by the variance of the pixels.\n",
    "  #float_image = tf.image.per_image_whitening(resized_image)\n",
    "  float_image = tf.image.per_image_standardization(resized_image)\n",
    "\n",
    "  # Ensure that the random shuffling has good mixing properties.\n",
    "  min_fraction_of_examples_in_queue = 0.4\n",
    "  min_queue_examples = int(num_examples_per_epoch *\n",
    "                           min_fraction_of_examples_in_queue)\n",
    "\n",
    "  # Generate a batch of images and labels by building up a queue of examples.\n",
    "  images, labels = _generate_image_and_label_batch(float_image, read_input.label,\n",
    "                                                   min_queue_examples, batch_size)\n",
    "\n",
    "  #tf.image_summary('val_images', images)\n",
    "  tf.summary.image('val_images', images)\n",
    "\n",
    "  return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '../Datasets/CIFAR-10/cifar-10-batches-bin'\n",
    "val_images, val_labels = inputs(eval_data=True, data_dir=data_dir, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'shuffle_batch:0' shape=(100, 24, 24, 3) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_2:0' shape=(100,) dtype=int32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(val_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
