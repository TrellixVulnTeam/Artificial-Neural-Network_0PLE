{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://danijar.com/variable-sequence-lengths-in-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Sequence Lengths in TensorFlow\n",
    "I recently wrote a guide on recurrent networks in TensorFlow: https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/\n",
    "That covered the basics but often we want to learn on sequences of variable lengths, possibly even within the same batch of training examples.\n",
    "In this post, I will explain how to use variable length sequences in TensorFlow and what implications they have on your model.\n",
    "\n",
    "# Computing the Sequence Length\n",
    "Since TensorFlow unfolds our recurrent network for a given number of steps, we can only feed sequences of that shape to the network.\n",
    "We also want the input to have a fixed size so that we can represent a training batch as a single tensor of shape `batch size x max length x features`.\n",
    "\n",
    "I will assume that the sequences are padded with zero vectors to fill up the remaining time steps in the batch.\n",
    "To pass sequence lengths to TensorFlow, we have to compute them from the batch.\n",
    "While we could do this in Numpy in a pre-processing step, let’s do it on the fly as part of the compute graph!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "    length = tf.reduce_sum(used, 1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first collapse the frame vectors (third dimension of a batch) into scalars using maximum.\n",
    "Each sequence is now a vector of scalars that will be zero for the padded frames at the end.\n",
    "We then use tf.sign() to convert the actual frames from their maximum values to values of one.\n",
    "<u>This gives us a binary mask of ones for used frames and zeros for unused frames that we can just sum to get the sequence length.</u>\n",
    "\n",
    "# Using the Length Information\n",
    "Now that we have a vector holding the sequence lengths, we can pass that to `dynamic_rnn()`, the function that unfolds our network, using the optional `sequence_length` parameter.\n",
    "When running the model later, TensorFlow will return zero vectors for states and outputs after these sequence lengths.\n",
    "Therefore, weights will not affect those outputs and don’t get trained on them."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "max_length = 100\n",
    "frame_size = 64\n",
    "num_hidden = 200\n",
    "\n",
    "sequence = tf.placeholder(tf.float32, [None, max_length, frame_size])\n",
    "\n",
    "output, state = tf.nn.dynamic_rnn(\n",
    "    tf.contrib.rnn.GRUCell(num_hidden),\n",
    "    sequence,\n",
    "    dtype=tf.float32,\n",
    "    sequence_length=length(sequence),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sequence_temp = tf.placeholder(tf.float32, [None, max_length, frame_size])\n",
    "print(sequence_temp)\n",
    "\n",
    "print(tf.abs(sequence_temp))\n",
    "print(tf.reduce_max(tf.abs(sequence_temp), 2))\n",
    "used_temp = tf.sign(tf.reduce_max(tf.abs(sequence_temp), 2))\n",
    "print(used_temp)\n",
    "length_temp = tf.reduce_sum(used_temp, 1)\n",
    "print(length_temp)\n",
    "length_temp = tf.cast(length_temp, tf.int32)\n",
    "print(length_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking the Cost Function\n",
    "Note that our output will still be of size `batch_size x max_length x out_size`, but with the last being zero vectors for sequences shorter than the maximum length.\n",
    "When you use the outputs at each time step, as in sequence labeling, we don’t want to consider them in our cost function.\n",
    "We mask out the unused frames and compute the mean error over the sequence length by dividing by the actual length.\n",
    "Using `tf.reduce_mean()` does not work here because it would devide by the maximum sequence length."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def cost(output, target):\n",
    "  # Compute cross entropy for each frame.\n",
    "  cross_entropy = target * tf.log(output)\n",
    "  cross_entropy = -tf.reduce_sum(cross_entropy, 2)\n",
    "  mask = tf.sign(tf.reduce_max(tf.abs(target), 2))\n",
    "  cross_entropy *= mask\n",
    "  # Average over actual sequence lengths.\n",
    "  cross_entropy = tf.reduce_sum(cross_entropy, 1)\n",
    "  cross_entropy /= tf.reduce_sum(mask, 1)\n",
    "  return tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compute the average of your error function the same way.\n",
    "Actually, we wouldn’t have to do the masking for the cost and error functions because both prediction and target are zero vectors for the padding frames so they are perfect predictions.\n",
    "Anyway, it’s nice to be explicit in code.\n",
    "Here is a full example of variable-length sequence labeling.\n",
    "\n",
    "# Select the Last Relevant Output\n",
    "For sequence classification, we want to feed the last output of the recurrent network into a predictor, e.g. a softmax layer.\n",
    "While taking the last frame worked well for fixed-sized sequences, we not have to select the last relevant frame.\n",
    "This is a bit cumbersome in TensorFlow since it does’t support advanced slicing yet.\n",
    "In Numpy this would just be `output[:, length - 1]`.\n",
    "But we need the indexing to be part of the compute graph in order to train the whole system end-to-end."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def last_relevant(output, length):\n",
    "  batch_size = tf.shape(output)[0]\n",
    "  max_length = tf.shape(output)[1]\n",
    "  out_size = int(output.get_shape()[2])\n",
    "  index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "  flat = tf.reshape(output, [-1, out_size])\n",
    "  relevant = tf.gather(flat, index)\n",
    "  return relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens here?\n",
    "We flatten the output tensor to shape frames in all `examples x output size`.\n",
    "Then we construct an index into that by creating a tensor with the start indices for each example `tf.range(0, batch_size) * max_length` and add the individual sequence lengths to it.\n",
    "`tf.gather()` then performs the actual indexing.\n",
    "Let’s hope the TensorFlow guys can provide proper indexing soon so this gets much easier.\n",
    "\n",
    "On a side node: A one-layer GRU network outputs its full state.\n",
    "In that case, we can use the state returned by `tf.nn.dynamic_rnn()` directly.\n",
    "Similarly, we can use state.o for a one-layer LSTM network.\n",
    "For more complex architectures, that doesn’t work or at least result in a large amount of parameters.\n",
    "\n",
    "We got the last relevant output and can feed that into a simple softmax layer to predict the class of each sequence:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_classes = 10\n",
    "\n",
    "last = last_relevant(output)\n",
    "weight = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden, num_classes], stddev=0.1))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can of course use more complex predictors with multiple layers as well.\n",
    "Here is the working example for variable-length sequence classification.\n",
    "\n",
    "I explained how to use recurrent networks on variable-length sequences and how to use their outputs. Feel free to comment with questions and remarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda2\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 error 45.2%\n",
      "Epoch  2 error 25.0%\n",
      "Epoch  3 error 15.6%\n",
      "Epoch  4 error 14.9%\n",
      "Epoch  5 error 10.1%\n",
      "Epoch  6 error 11.4%\n",
      "Epoch  7 error 9.7%\n",
      "Epoch  8 error 7.8%\n",
      "Epoch  9 error 6.5%\n",
      "Epoch 10 error 6.5%\n"
     ]
    }
   ],
   "source": [
    "# Updated to work with TF 1.4: https://gist.github.com/abaybektursun/98656e483ec6e918c26235b47f3f5d60\n",
    "# Working example for my blog post at:\n",
    "# http://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "import functools\n",
    "import sets\n",
    "import tensorflow as tf\n",
    "from tensorflow import nn\n",
    "\n",
    "\n",
    "def lazy_property(function):\n",
    "    attribute = '_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def wrapper(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class VariableSequenceClassification:\n",
    "\n",
    "    def __init__(self, data, target, num_hidden=200, num_layers=2):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self._num_hidden = num_hidden\n",
    "        self._num_layers = num_layers\n",
    "        self.prediction\n",
    "        self.error\n",
    "        self.optimize\n",
    "\n",
    "    @lazy_property\n",
    "    def length(self):\n",
    "        used = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "        length = tf.reduce_sum(used, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        # Recurrent network.\n",
    "        output, _ = nn.dynamic_rnn(\n",
    "            nn.rnn_cell.GRUCell(self._num_hidden),\n",
    "            data,\n",
    "            dtype=tf.float32,\n",
    "            sequence_length=self.length,\n",
    "        )\n",
    "        last = self._last_relevant(output, self.length)\n",
    "        # Softmax layer.\n",
    "        weight, bias = self._weight_and_bias(\n",
    "            self._num_hidden, int(self.target.get_shape()[1]))\n",
    "        prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)\n",
    "        return prediction\n",
    "\n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        cross_entropy = -tf.reduce_sum(self.target * tf.log(self.prediction))\n",
    "        return cross_entropy\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        learning_rate = 0.003\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "        return optimizer.minimize(self.cost)\n",
    "\n",
    "    @lazy_property\n",
    "    def error(self):\n",
    "        mistakes = tf.not_equal(\n",
    "            tf.argmax(self.target, 1), tf.argmax(self.prediction, 1))\n",
    "        return tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "\n",
    "    @staticmethod\n",
    "    def _weight_and_bias(in_size, out_size):\n",
    "        weight = tf.truncated_normal([in_size, out_size], stddev=0.01)\n",
    "        bias = tf.constant(0.1, shape=[out_size])\n",
    "        return tf.Variable(weight), tf.Variable(bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def _last_relevant(output, length):\n",
    "        batch_size = tf.shape(output)[0]\n",
    "        max_length = int(output.get_shape()[1])\n",
    "        output_size = int(output.get_shape()[2])\n",
    "        index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "        flat = tf.reshape(output, [-1, output_size])\n",
    "        relevant = tf.gather(flat, index)\n",
    "        return relevant\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # We treat images as sequences of pixel rows.\n",
    "    train, test = sets.Mnist()\n",
    "    _, rows, row_size = train.data.shape\n",
    "    num_classes = train.target.shape[1]\n",
    "    \n",
    "    data = tf.placeholder(tf.float32, [None, rows, row_size])\n",
    "    target = tf.placeholder(tf.float32, [None, num_classes])\n",
    "    \n",
    "    model = VariableSequenceClassification(data, target)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(10):\n",
    "        for _ in range(100):\n",
    "            batch = train.sample(10)\n",
    "            sess.run(model.optimize, {data: batch.data, target: batch.target})\n",
    "        error = sess.run(model.error, {data: test.data, target: test.target})\n",
    "        print('Epoch {:2d} error {:3.1f}%'.format(epoch + 1, 100 * error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
