{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def time_taken(start, end):\n",
    "    \"\"\"Human readable time between `start` and `end`\n",
    "\n",
    "    :param start: time.time()\n",
    "    :param end: time.time()\n",
    "    :returns: day:hour:minute:second.millisecond\n",
    "    \"\"\"\n",
    "    my_time = end-start\n",
    "    day = my_time // (24 * 3600)\n",
    "    my_time = my_time % (24 * 3600)\n",
    "    hour = my_time // 3600\n",
    "    my_time %= 3600\n",
    "    minutes = my_time // 60\n",
    "    my_time %= 60\n",
    "    seconds = my_time\n",
    "    milliseconds = ((end - start)-int(end - start))\n",
    "    day_hour_min_sec = str('%02d' % int(day))+\":\"+str('%02d' % int(hour))+\":\"+str('%02d' % int(minutes))+\":\"+str('%02d' % int(seconds)+\".\"+str('%.3f' % milliseconds)[2:])\n",
    "    return day_hour_min_sec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Works on Python 3\n",
    "Now the original objects can be retrieved (\"loaded\") from _.bin in the same order as they were written (\"dumped\") into it\n",
    "\"\"\"\n",
    "\n",
    "db_file_name = r\"C:\\Users\\GyanT\\Documents\\GitHub\\BrainSpec\\ML\\INTERPRET_LC\\Database\\INTERPRET_Validated_DB.bin\"\n",
    "\n",
    "bin_file = open(db_file_name, \"rb\")\n",
    "\n",
    "features = pickle.load(bin_file)\n",
    "lables = pickle.load(bin_file)\n",
    "bi_lables = pickle.load(bin_file)\n",
    "db_info = pickle.load(bin_file)\n",
    "tumour_classes = pickle.load(bin_file)\n",
    "\n",
    "bin_file.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Works on Python 3\n",
    "Now the original objects can be retrieved (\"loaded\") from _.bin in the same order as they were written (\"dumped\") into it\n",
    "\"\"\"\n",
    "\n",
    "db_file_name = r\"C:\\Users\\GyanT\\Documents\\GitHub\\BrainSpec\\ML\\INTERPRET_LC\\Database\\DOD_DB.bin\"\n",
    "\n",
    "bin_file = open(db_file_name, \"rb\")\n",
    "\n",
    "dod_features = pickle.load(bin_file)\n",
    "dod_bi_lables = pickle.load(bin_file)\n",
    "\n",
    "bin_file.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "features = list(features) + list(dod_features)\n",
    "bi_lables = list(bi_lables) + list(dod_bi_lables)\n",
    "\n",
    "features = np.array(features)\n",
    "bi_lables = np.array(bi_lables)\n",
    "\n",
    "print(features.shape)\n",
    "print(bi_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304, 1026)\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Works on Python 3\n",
    "Now the original objects can be retrieved (\"loaded\") from _.bin in the same order as they were written (\"dumped\") into it\n",
    "\"\"\"\n",
    "\n",
    "db_file_name = r\"C:\\Users\\GyanT\\Documents\\GitHub\\BrainSpec\\ML\\INTERPRET_LC\\Database\\INTERPRET_DOD_DB.bin\"\n",
    "\n",
    "bin_file = open(db_file_name, \"rb\")\n",
    "\n",
    "features = pickle.load(bin_file)\n",
    "bi_lables = pickle.load(bin_file)\n",
    "\n",
    "bin_file.close()\n",
    "\n",
    "print(features.shape)\n",
    "print(bi_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112  73 263 182  81 254  24   3  64 160 167 277 211 190 300 186 210 232\n",
      "  84 129 152 272 169 293 165 220  16 296 193  99 204  74 100 175 171 208\n",
      " 118 117 291 228 275  37 183 137   6  94 301 174 131 238 295  38 127 270\n",
      "  93  67 240  61 302 218 120 138 230  32 189 168   9  25  35 268 299 243\n",
      "  58 134 196 236  45  96 142 163  69 237 265   0 158 143  89 173  66 233\n",
      " 195   2  30 259 253 206   1 286  77 146  56  11 199 216 197 280 188 187\n",
      "  19 139 104 223 246 250 148 239 269 282  18 290  68 273 145  31 113 144\n",
      "  15  52 121 194 205 153 241 147 106 109 279 281  83 170 119 162 191 125\n",
      " 221 166  78  20 132  29 105 242  17   7 115 107 126 283 249 198 285 266\n",
      "  40   4  82 222 192 297  46 203 207  57  14  13  72 133 262  50 212 256\n",
      "  85  26 172 271  53 251 227 231 201  95  87 229 225 179 122 276 234 226\n",
      " 161  28  36 213  54 111  12  27 245  80 123 284 288 102 200 155 209  48\n",
      " 178 176  71 114 261  70 110 202  10  41 244 130 150   5  21  39  90 180\n",
      " 294 214 116  88  97 184  33 124 219 289 156  86  92 255 140  98 135 136\n",
      "  55 303 128 247 287 154  42  43  79 257 157  76 267  60 185 217  51 298\n",
      " 159 278 260 258 274 224 151  49  75 101 141 103  34 235 177  63 164  44\n",
      "  59  22 108 248 149  47  62 264   8 292  91 215  23 252  65 181]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "shuffle_indices = np.arange(len(features))\n",
    "np.random.shuffle(shuffle_indices)\n",
    "    \n",
    "print(shuffle_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(len(features)*0.75)\n",
    "\n",
    "X_train, X_test = features[shuffle_indices][0:split_point], features[shuffle_indices][split_point:]\n",
    "y_train, y_test = bi_lables[shuffle_indices][0:split_point], bi_lables[shuffle_indices][split_point:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 228 TEST: 76\n",
      "\n",
      "TRAIN: 228 TEST: 76\n",
      "\n",
      "TRAIN: 228 TEST: 76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "y_lables = bi_lables\n",
    "ss = ShuffleSplit(n_splits=3, test_size=0.25, random_state=0)\n",
    "\n",
    "for train_index, test_index in ss.split(features):\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = y_lables[train_index], y_lables[test_index]\n",
    "    #print(\"TRAIN Y:\", y_train)\n",
    "    #print(\"TEST Y:\", y_test)\n",
    "    #print(\"KNN: \", knn_score(X_train, X_test, y_train, y_test))\n",
    "    #print(\"SVM: \", svm_score(X_train, X_test, y_train, y_test))\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = max(y_lables)\n",
    "\n",
    "if num_classes > 2:\n",
    "    num_classes = len(tumour_classes)\n",
    "else: # binary\n",
    "    num_classes = 2\n",
    "\n",
    "if num_classes == len(tumour_classes):\n",
    "    y_train = label_binarize(y_train, classes=range(len(tumour_classes)))\n",
    "    y_test = label_binarize(y_test, classes=range(len(tumour_classes)))\n",
    "else: # binary\n",
    "    y_train = label_binarize(y_train, classes=range(3))[:, 0:2]\n",
    "    y_test = label_binarize(y_test, classes=range(3))[:, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    #features=dict(features)\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((?, 1026), (?, 2)), types: (tf.float64, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_fn(X_train, y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((?, 1026), (?, 2)), types: (tf.float64, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_input_fn(X_test, y_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(228, 1026)\n",
      "(228, 1, 1026)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[:, tf.newaxis].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'interpret/nn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002923923C550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "def my_model(features, labels, mode, params):\n",
    "    print(features, labels, mode, params)\n",
    "    \n",
    "    net = tf.layers.dense(inputs=features, units=1024, activation=tf.nn.relu)\n",
    "    print(net)\n",
    "    net = tf.layers.dense(inputs=net, units=512, activation=tf.nn.relu)\n",
    "    print(net)\n",
    "    net = tf.layers.dense(inputs=net, units=256, activation=tf.nn.relu)\n",
    "    print(net)\n",
    "    net = tf.layers.dense(inputs=net, units=128, activation=tf.nn.relu)\n",
    "    print(net)\n",
    "    logits = tf.layers.dense(net, params['n_classes'], name='logits')\n",
    "    print(\"logits: \", logits)\n",
    "\n",
    "    # Compute predictions.\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    \n",
    "    \n",
    "    # Compute loss.\n",
    "    #loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "\n",
    "    # Compute evaluation metrics.\n",
    "#     accuracy = tf.metrics.accuracy(labels=labels,\n",
    "#                                    predictions=predicted_classes,\n",
    "#                                    name='acc_op')\n",
    "    \n",
    "    labels_new = tf.argmax(labels, 1)\n",
    "    accuracy = tf.metrics.accuracy(labels=labels_new,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "    \n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "    #tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        print(mode, loss, metrics)\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # Create training op.\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    #optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    \n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "my_checkpointing_config = tf.estimator.RunConfig(\n",
    "    #log_step_count_steps = 1,\n",
    "    keep_checkpoint_max=1,\n",
    ")\n",
    "\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=my_model,\n",
    "    params={\n",
    "        'n_classes': num_classes,\n",
    "    },\n",
    "    model_dir='interpret/nn',\n",
    "    config=my_checkpointing_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "Tensor(\"IteratorGetNext:0\", shape=(?, 1026), dtype=float64) Tensor(\"IteratorGetNext:1\", shape=(?, 2), dtype=int32) train {'n_classes': 2}\n",
      "Tensor(\"dense/Relu:0\", shape=(?, 1024), dtype=float64)\n",
      "Tensor(\"dense_1/Relu:0\", shape=(?, 512), dtype=float64)\n",
      "Tensor(\"dense_2/Relu:0\", shape=(?, 256), dtype=float64)\n",
      "Tensor(\"dense_3/Relu:0\", shape=(?, 128), dtype=float64)\n",
      "logits:  Tensor(\"logits/BiasAdd:0\", shape=(?, 2), dtype=float64)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into interpret/nn\\model.ckpt.\n",
      "INFO:tensorflow:loss = 44036.25620311164, step = 0\n",
      "INFO:tensorflow:global_step/sec: 16.4037\n",
      "INFO:tensorflow:loss = 8.305367174884118, step = 100 (6.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.9071\n",
      "INFO:tensorflow:loss = 9.252251514604604, step = 200 (5.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.799\n",
      "INFO:tensorflow:loss = 1.2259621592485543, step = 300 (5.953 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.7573\n",
      "INFO:tensorflow:loss = 3.7156738989347313, step = 400 (5.968 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.732\n",
      "INFO:tensorflow:loss = 1.0285673870172798, step = 500 (5.976 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.8136\n",
      "INFO:tensorflow:loss = 0.1727824338792996, step = 600 (5.949 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.7153\n",
      "INFO:tensorflow:loss = 0.04734333004001437, step = 700 (5.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.6181\n",
      "INFO:tensorflow:loss = 0.0017980391149324817, step = 800 (6.019 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.6541\n",
      "INFO:tensorflow:loss = 0.0008213499642921102, step = 900 (6.004 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into interpret/nn\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.000521607769426137.\n",
      "day, hour, minute, second.millisecond-> 00:00:01:32.309\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with open('interpret/nn/Training_Time.txt', 'w') as file:\n",
    "    file.write('Training Time:\\n')\n",
    "    \n",
    "batch = len(X_train)\n",
    "training_epochs = 1000\n",
    "\n",
    "#category_label_train = category_label_train-1\n",
    "# Train the Model.\n",
    "# classifier.train(input_fn=lambda:train_input_fn(audio_frames_2d_train, category_label_train, batch), steps=training_epochs)\n",
    "classifier.train(input_fn=lambda:train_input_fn(X_train, y_train, batch), steps=training_epochs)\n",
    "\n",
    "\n",
    "training_time = time_taken(start_time, time.time())\n",
    "file = open('interpret/nn/Training_Time.txt', 'a') # append to the file created\n",
    "file.write(training_time+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(\"day, hour, minute, second.millisecond->\", training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "Tensor(\"IteratorGetNext:0\", shape=(?, 1026), dtype=float64) Tensor(\"IteratorGetNext:1\", shape=(?, 2), dtype=int32) eval {'n_classes': 2}\n",
      "Tensor(\"dense/Relu:0\", shape=(?, 1024), dtype=float64)\n",
      "Tensor(\"dense_1/Relu:0\", shape=(?, 512), dtype=float64)\n",
      "Tensor(\"dense_2/Relu:0\", shape=(?, 256), dtype=float64)\n",
      "Tensor(\"dense_3/Relu:0\", shape=(?, 128), dtype=float64)\n",
      "logits:  Tensor(\"logits/BiasAdd:0\", shape=(?, 2), dtype=float64)\n",
      "eval Tensor(\"Mean:0\", shape=(), dtype=float64) {'accuracy': (<tf.Tensor 'acc_op/value:0' shape=() dtype=float32>, <tf.Tensor 'acc_op/update_op:0' shape=() dtype=float32>)}\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-08-23-19:10:45\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from interpret/nn\\model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-08-23-19:10:46\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.9736842, global_step = 1000, loss = 0.53370273\n",
      "\n",
      "Test set accuracy: 0.974\n",
      "\n",
      "day, hour, minute, second.millisecond-> 00:00:01:33.458\n"
     ]
    }
   ],
   "source": [
    "batch = len(X_train)\n",
    "\n",
    "#category_label_test = category_label_test-1\n",
    "# Evaluate the model.\n",
    "#eval_result = classifier.evaluate(input_fn=lambda:eval_input_fn(audio_frames_2d_test, category_label_test, batch))\n",
    "eval_result = classifier.evaluate(input_fn=lambda:eval_input_fn(X_test, y_test, batch))\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
    "\n",
    "\n",
    "training_time = time_taken(start_time, time.time())\n",
    "file = open('interpret/nn/Training_Time.txt', 'a') # append to the file created\n",
    "file.write(\"Evaluation Time:\"+\"\\n\")\n",
    "file.write(training_time+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(\"day, hour, minute, second.millisecond->\", training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.000e+00,  4.170e-01,  0.000e+00, ..., -2.314e+04, -2.467e+04,\n",
      "       -2.619e+04]), array([1.65000000e-01, 2.01500000e+00, 5.27300000e+00, ...,\n",
      "       9.84491628e+05, 9.75253058e+05, 9.65189182e+05])]\n",
      "[array([0, 1]), array([1, 0])]\n"
     ]
    }
   ],
   "source": [
    "temp_array = []\n",
    "\n",
    "temp_array.append(X_test[0])\n",
    "temp_array.append(X_test[1])\n",
    "print(temp_array)\n",
    "temp_array = np.array(temp_array)\n",
    "\n",
    "\n",
    "temp_array2 = []\n",
    "\n",
    "temp_array2.append(y_test[0])\n",
    "temp_array2.append(y_test[1])\n",
    "print(temp_array2)\n",
    "temp_array2 = np.array(temp_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(input_fn=lambda:eval_input_fn(temp_array, labels=None, batch_size=batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "Tensor(\"IteratorGetNext:0\", shape=(?, 1026), dtype=float64) None infer {'n_classes': 2}\n",
      "Tensor(\"dense/Relu:0\", shape=(?, 1024), dtype=float64)\n",
      "Tensor(\"dense_1/Relu:0\", shape=(?, 512), dtype=float64)\n",
      "Tensor(\"dense_2/Relu:0\", shape=(?, 256), dtype=float64)\n",
      "Tensor(\"dense_3/Relu:0\", shape=(?, 128), dtype=float64)\n",
      "logits:  Tensor(\"logits/BiasAdd:0\", shape=(?, 2), dtype=float64)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from interpret/nn\\model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "\n",
      "Prediction is \"TUMOR\" (100.0%)\n",
      "\n",
      "Prediction is \"NORMAL_TISSUE\" (100.0%)\n",
      "day, hour, minute, second.millisecond-> 00:00:01:34.188\n"
     ]
    }
   ],
   "source": [
    "CLASSES = ['NORMAL_TISSUE', 'TUMOR']\n",
    "template = ('\\nPrediction is \"{}\" ({:.1f}%)')\n",
    "\n",
    "for pred_dict in predictions:\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "        \n",
    "    print(template.format(CLASSES[class_id], 100 * probability))\n",
    "\n",
    "training_time = time_taken(start_time, time.time())\n",
    "file = open('interpret/nn/Training_Time.txt', 'a') # append to the file created\n",
    "file.write(\"Prediction Time:\"+\"\\n\")\n",
    "file.write(training_time+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(\"day, hour, minute, second.millisecond->\", training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
